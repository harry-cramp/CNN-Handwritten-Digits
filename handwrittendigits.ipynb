{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# (down)load MNIST handwritten digit dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# transform images into pixel arrays\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32')\n",
    "x_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32')\n",
    "\n",
    "# normalise pixel values to scale of 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# outputs in one hot encode form\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # neural network with one hidden layer\n",
    "    # contains 784 (28x28) neurons, one for each input\n",
    "    # uses rectifier activation function for neurons in hidden layer\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    # softmax activation function used on output layer to turn outputs into probability values\n",
    "    # and selects one of the 10 classes as the model's prediction\n",
    "    # Logarithmic loss (keras: categorical_crossentropy) is used as the loss function\n",
    "    # and ADAM gradient descent is used to learn the weights\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = baseline_model()\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 [==============================] - 16s 53ms/step - loss: 0.2575 - accuracy: 0.9251 - val_loss: 0.0846 - val_accuracy: 0.9752\n",
      "Epoch 2/10\n",
      "300/300 [==============================] - 16s 53ms/step - loss: 0.0827 - accuracy: 0.9751 - val_loss: 0.0632 - val_accuracy: 0.9794\n",
      "Epoch 3/10\n",
      "300/300 [==============================] - 16s 53ms/step - loss: 0.0598 - accuracy: 0.9814 - val_loss: 0.0471 - val_accuracy: 0.9841\n",
      "Epoch 4/10\n",
      "300/300 [==============================] - 16s 54ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 0.0393 - val_accuracy: 0.9864\n",
      "Epoch 5/10\n",
      "300/300 [==============================] - 16s 54ms/step - loss: 0.0395 - accuracy: 0.9882 - val_loss: 0.0352 - val_accuracy: 0.9892\n",
      "Epoch 6/10\n",
      "300/300 [==============================] - 15s 50ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0364 - val_accuracy: 0.9879\n",
      "Epoch 7/10\n",
      "300/300 [==============================] - 16s 55ms/step - loss: 0.0270 - accuracy: 0.9914 - val_loss: 0.0345 - val_accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "300/300 [==============================] - 16s 53ms/step - loss: 0.0234 - accuracy: 0.9924 - val_loss: 0.0387 - val_accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "300/300 [==============================] - 16s 53ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.0392 - val_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "300/300 [==============================] - 15s 49ms/step - loss: 0.0181 - accuracy: 0.9944 - val_loss: 0.0377 - val_accuracy: 0.9881\n",
      "CNN Error: 1.19%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load digit data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# reshape to suit Keras form for CNN\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32')\n",
    "\n",
    "# normalise inputs\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# outputs in one hot encode form\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # convolutional layer with 32 feature maps sized 5x5 and a rectifier activation function - the input layer\n",
    "    model.add(Conv2D(32, (5, 5), input_shape = (28, 28, 1), activation='relu'))\n",
    "    # pooling layer using max pooling 2D function using pool size of 2x2\n",
    "    model.add(MaxPooling2D())\n",
    "    # regularisation layer using dropout, randomly excluding 20% of layer's neurons to reduce overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "    # flatten converts 2D matrix data to a vector, allowing it to be processed by standard layers\n",
    "    model.add(Flatten())\n",
    "    # fully connected layer with 128 neurons and a rectifier activation function\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # output layer has 10 neurons, one for each class, and a softmax activation function for probabilities\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = cnn_model()\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
